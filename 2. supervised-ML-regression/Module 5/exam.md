1.
Question 1
When working with regularization, what is the view that illuminates the actual optimization problem and shows why LASSO generally zeros out coefficients?



Analytical view


Geometric view


Probabilistic view


Regression view

2.
Question 2
When working with regularization, what is the view that recalibrates our understanding of LASSO and a Ridge, as a base problem, where coefficients have particular prior distributions? 



Probabilistic view


Geometric view


Analytical view


Regression view

3.
Question 3
When working with regularization, what is the logical view of how to achieve the goal of reducing complexity? 



Geometric view


Analytical view


Regression view


Probabilistic view

4.
Question 4
All of the following statements about Regularization are TRUE except:



Optimizing predictive models is about finding the right bias/variance tradeoff.


Features should rarely or never be scaled prior to implementing regularization.


We need models that are sufficiently complex to capture patterns in data, but not so complex that they overfit.


Regularization techniques have an analytical, a geometric, and a probabilistic interpretation.  

5.
Question 5
When working with regularization and using the geometric formulation, what is found at the intersection of the penalty boundary and a contour of the traditional OLS cost function surface?



The cost function minimum


A smaller range of coefficients


The prior distribution of Î²


A peaked density

6.
Question 6
Which statement under the Probabilistic View is correct?



Regularization imposes certain errors on the regression coefficients. Feedback: Incorrect! Please review the further Details of Regularization lessons. 

      


Regularization imposes certain priors on the regression coefficients. 

      


Regularization uses some regression coefficients to inflate the errors.     


Regularization coefficients do not take into consideration prior probabilities. 

7.
Question 7
Increasing L2/L1 penalties force coefficients to be smaller, restricting their plausible range. This statement is part of what View?

      



Geometric View


Probabilistic View    


Analytic View    

8.
Question 8
What does a higher lambda term mean in Regularization technique?

      



Higher lambda decreases variance, means smaller coefficients.     


Higher lambda increases variance, means smaller coefficients.     


Higher lambda decreases variance, means larger coefficients.     


Higher lambda decreases prior probability.     

9.
Question 9
What concept/s under Probabilistic View is/are True?

      



We can derive the posterior probability by knowing the probability of target and the prior distribution.

      


The prior distribution is derived from independent draws of a prior coefficient density function that we choose when regularizing.

      


L2 (ridge) regularization imposes a Gaussian prior on the coefficients, while L1 (lasso) regularization imposes a Laplacian prior.

      


All of the above

10.
Question 10
What statement is True?

      



We reduce the complexity of the model by minimizing the error on our training set. 

      


By penalizing the cost function, we increase the complexity of the model.     


The goal of Regularization is always going to be to optimize our complexity trade off, so we can minimize error on the hold-out set.     


Introducing Regularization will increase bias and variance.      